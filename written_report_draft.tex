\documentclass[11pt]{article}

\usepackage[margin=1in]{geometry}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{longtable}
\usepackage{array}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{xcolor}
\usepackage{hyperref}
\usepackage[authoryear,round]{natbib}
\usepackage{float}
\usepackage{enumitem}
\usepackage{listings}
\usepackage{tikz}
\usetikzlibrary{arrows.meta,positioning,shapes.geometric,calc}

\hypersetup{
  colorlinks=true,
  linkcolor=blue,
  urlcolor=blue,
  citecolor=blue
}

% --- Rust listings (minimal, no external deps) ---
\lstdefinelanguage{Rust}{
  keywords={as, break, const, continue, crate, else, enum, extern, false, fn, for, if, impl, in, let, loop, match, mod, move, mut, pub, ref, return, self, Self, static, struct, super, trait, true, type, unsafe, use, where, while, async, await, dyn},
  sensitive=true,
  comment=[l]{//},
  morecomment=[s]{/*}{*/},
  morestring=[b]",
}

\lstset{
  language=Rust,
  basicstyle=\ttfamily\small,
  columns=fullflexible,
  keepspaces=true,
  frame=single,
  breaklines=true,
  showstringspaces=false,
  keywordstyle=\bfseries\color{blue!60!black},
  commentstyle=\color{green!40!black},
  stringstyle=\color{red!60!black}
}

\title{Medical Care Robot Coordination System: Written Report}
\author{FA, Haochen\\Student ID: 24113347D\\COMP2432 Operating Systems}
\date{25/26 Sem 2}

\begin{document}
\maketitle

\begin{abstract}
Medical Care Robot Coordination System (MCRoCS, ``McRocks'') is a lightweight operating-system core for coordinating multiple medical-care robots with safe concurrency. The system implements exactly three required services: a thread-safe task queue, exclusive zone access control, and a heartbeat-based health monitor. The main challenges were avoiding race conditions while keeping critical sections small and ensuring that offline detection was observable in a short demo. The solution uses Rust's \texttt{Mutex} and \texttt{Condvar} to serialize access to shared state, plus atomics for lightweight counters inside the simulation \citep{rustmutex,rustcondvar}. A CLI-driven demo shows multiple robots fetching tasks concurrently, entering zones without overlap, and a robot timing out after it stops heartbeats. Benchmarks and stress sweeps report throughput, zone wait times, and CPU usage, while tests validate single-consumer task behavior, zone exclusivity, and timeout detection. The result is a minimal, correct coordination layer aligned with the project's OS learning outcomes.
\end{abstract}

\section{Introduction}
Modern hospitals increasingly rely on autonomous robots for delivery, sanitation, and assistance. In such settings, coordination errors are not merely inconveniences; they can cause safety incidents, resource contention, or service delays. Medical Care Robot Coordination System addresses this by building a minimal operating-system core that enforces safe concurrency without introducing complex scheduling or preemption. The goal is not to model a complete robot control stack but to implement and demonstrate three essential coordination services under concurrent load: a task queue that ensures each task is assigned at most once, a zone access controller that prevents two robots from occupying the same physical zone simultaneously, and a health monitor that detects robots that stop sending heartbeats.

The project objectives are closely tied to operating-systems fundamentals. First, it must implement correct concurrency control so that shared state remains consistent under multiple threads. Second, it must use explicit synchronization to prevent race conditions. Third, it must coordinate the interactions among threads to produce a predictable, observable outcome that can be demonstrated in a short runtime. The minimal scope is intentional: by removing preemption and complex scheduling policies, the design keeps focus on safety invariants and synchronization correctness. This also makes it possible to reason about system state transitions and to write tests that are both meaningful and deterministic.

The implementation is a Rust Cargo project organized around three core modules, plus a simulation driver. Each component uses safe synchronization primitives (\texttt{Mutex}, \texttt{Condvar}, and atomics) and follows a narrow-lock-scope design to minimize contention and avoid deadlocks. Rust's ownership model and standard library concurrency support help enforce data-race freedom by construction \citep{klabnik2022}. The simulation spawns multiple robot threads, drives task consumption, enforces zone ownership, and periodically checks for offline robots. The command-line interface supports a default demo as well as benchmark and stress modes that emit CSV metrics for analysis. This report details the design, implementation, and evaluation of Medical Care Robot Coordination System (MCRoCS) and connects the observed behaviors to OS-level concurrency concepts.

\section{Related Works}
Concurrency control has been a central operating-systems concern since early multiprogramming. Seminal work on mutual exclusion and synchronization, including Dijkstra's and Lamport's classic algorithms, established the need for explicit coordination when multiple execution contexts share resources \citep{dijkstra1965,lamport1978}. Practical guidance on programming with threads reinforced these principles for real systems \citep{birrell1989}. Monitors and condition variables, popularized in systems like Mesa, formalized a practical pattern: a shared state protected by a lock, with threads waiting on conditions that are re-checked after waking \citep{hoare1974,lampson1980}. Medical Care Robot Coordination System uses this pattern directly, with \texttt{Mutex}-guarded state and \texttt{Condvar} waits for both task queues and zone access \citep{rustmutex,rustcondvar}. This mirrors the approach described in modern OS texts that emphasize correct locking, short critical sections, and explicit wait conditions \citep{arpaci2018,silberschatz2018,tanenbaum2015}.

Task queues are a common concurrency primitive in systems software, appearing in thread pools, work-stealing schedulers, and I/O dispatch loops \citep{arpaci2018,silberschatz2018}. In many high-performance systems, lock-free or multi-queue designs reduce contention, but they are more complex to reason about \citep{herlihy2020}. For the project, the requirement is correctness and clarity rather than maximum throughput. A single FIFO queue guarded by a mutex and condition variable provides strong safety properties: tasks are consumed at most once, and consumers can block efficiently until work arrives \citep{hoare1974}. This is consistent with the ``producer-consumer'' model in classical synchronization literature and is a well-understood pattern for teaching concurrency \citep{downey2016}. Modern practitioner-focused texts likewise emphasize explicit synchronization for shared state in concurrent programs \citep{williams2019}.

Resource coordination often reduces to mutual exclusion on a namespace of shared resources \citep{silberschatz2018,tanenbaum2015}. The zone controller in the project is conceptually similar to a lock table or resource allocator: each zone has at most one owner at a time \citep{gray1993}. This resembles database lock managers and OS-level resource maps where ownership must be unambiguous. Using a hash map from zone ID to robot ID makes the ownership explicit and easy to audit. The use of a condition variable to wait for zone availability aligns with monitor-based coordination patterns \citep{hoare1974,lampson1980}. It is also consistent with the trade-offs discussed in modern concurrency references: coarse-grained locking simplifies correctness reasoning but can limit parallelism if the lock is held too long \citep{herlihy2020}. In the project, each zone acquisition holds the lock only long enough to check and update the ownership map, minimizing contention.

Health monitoring through heartbeats is common in distributed systems and fault-tolerant designs \citep{lynch1996,chandra1996}. In such systems, nodes periodically emit a signal that a watcher uses to infer liveness. Failure detection is imperfect and depends on timeout selection; shorter timeouts detect failures faster but increase false positives \citep{lynch1996}. The project adopts a simple timeout-based scheme suitable for a single-process simulation. The monitor maintains a last-seen timestamp per robot and marks robots offline once their heartbeat exceeds a configured duration. This is consistent with classic failure detector models and highlights the OS-level idea of watchdog timers \citep{chandra1996}.

Compared with more sophisticated approaches---such as lock-free queues, adaptive timeouts, or priority-aware scheduling---the project intentionally chooses minimal mechanisms. This aligns with the project's pedagogical goals: by using straightforward, well-documented primitives, it becomes easier to explain the safety invariants and to provide testable evidence of correctness. The design therefore emphasizes clarity and correctness over performance, while still enabling quantitative benchmarking.

\section{Implementation}
\subsection{Architecture overview}
Medical Care Robot Coordination System (MCRoCS) is organized into four main modules and a CLI entry point. The core modules implement the task queue, zone access control, and health monitoring. The simulation driver (\texttt{sim.rs}) wires these together, spawns robot threads, and collects metrics. Logging is gated to debug builds to keep release output clean and benchmark-friendly.

\begin{figure}[H]
  \centering
  \begin{tikzpicture}[
    node distance=9mm and 14mm,
    box/.style={draw, rounded corners, align=center, inner sep=4pt, font=\small},
    arr/.style={-{Latex[length=2mm]}, thick}
  ]
    \node[box] (cli) {CLI entry\\\texttt{main.rs}};
    \node[box, right=of cli] (sim) {Simulation runner\\\texttt{sim.rs}};

    \node[box, below left=of sim] (tq) {TaskQueue};
    \node[box, below=of sim] (za) {ZoneAccess};
    \node[box, below right=of sim] (hm) {HealthMonitor};
    \node[box, below=of za] (log) {Dev logging\\macro};

    \node[box, below=of tq] (tqsync) {Mutex + Condvar};
    \node[box, below=of hm] (hmsync) {Mutex + HashMap/HashSet};
    \node[box, below=of za, yshift=-14mm] (zasync) {Mutex + Condvar};

    \draw[arr] (cli) -- (sim);
    \draw[arr] (sim) -- (tq);
    \draw[arr] (sim) -- (za);
    \draw[arr] (sim) -- (hm);
    \draw[arr] (sim) -- (log);

    \draw[arr] (tq) -- (tqsync);
    \draw[arr] (za) -- (zasync);
    \draw[arr] (hm) -- (hmsync);
  \end{tikzpicture}
  \caption{Architecture overview of the Medical Care Robot Coordination System (CLI, simulation runner, and three core services).}
  \label{fig:architecture}
\end{figure}

\subsection{Task queue}
The task queue uses a \texttt{Mutex<\allowbreak VecDeque<\allowbreak Task>>} to serialize access to an internal FIFO, and a \texttt{Condvar} to block consumers when the queue is empty \citep{rustmutex,rustcondvar}. This design ensures that each task is consumed at most once, and it prevents busy waiting. The critical section is minimal: a consumer holds the lock only long enough to check for a task and pop it. When the queue is empty, the thread waits on the condition variable, which releases the lock and re-acquires it on wake.

\noindent\textbf{Critical code path (blocking pop):}
\begin{lstlisting}
pub fn pop_blocking_or_closed(&self) -> Option<Task> {
    let mut guard = self.inner.lock().expect("task queue mutex poisoned");
    loop {
        if let Some(task) = guard.queue.pop_front() {
            return Some(task);
        }
        if guard.closed {
            return None;
        }
        guard = self.available.wait(guard).expect("condvar wait failed");
    }
}
\end{lstlisting}

The queue also supports \texttt{try\_pop} for non-blocking access and a \texttt{close} operation that wakes all blocked consumers. Tests validate that tasks are consumed exactly once across multiple threads, that blocking consumers wake on push, and that closure unblocks waiting threads. These tests directly target the invariants required by the project: single assignment of tasks and safe concurrent access.

\subsection{Zone access control}
The zone controller enforces exclusive occupancy per zone using a \texttt{Mutex<HashMap<ZoneId, RobotId>>} and a \texttt{Condvar} \citep{rustmutex,rustcondvar}. Each robot requests a zone and blocks until the zone is free. When the zone becomes available, the robot becomes the owner in the map and proceeds. Release removes the owner entry and notifies waiters.

\noindent\textbf{Critical code path (acquire):}
\begin{lstlisting}
pub fn acquire(&self, zone: ZoneId, robot: RobotId) {
    let mut guard = self.occupied.lock().expect("zone mutex poisoned");
    loop {
        if !guard.contains_key(&zone) {
            guard.insert(zone, robot);
            return;
        }
        guard = self.available.wait(guard).expect("condvar wait failed");
    }
}
\end{lstlisting}

This approach keeps the lock scope short and avoids deadlocks because only one lock is held at a time. The design is intentionally conservative: it prioritizes correctness and explicit ownership tracking. Unit tests spawn multiple contenders for a single zone and verify that the observed occupancy never exceeds one. In release builds, non-owner release attempts are logged and rejected, preserving safety even without debug assertions.

\begin{figure}[H]
  \centering
  \begin{tikzpicture}[
    node distance=9mm and 12mm,
    box/.style={draw, rounded corners, align=center, inner sep=4pt, font=\small},
    decision/.style={draw, diamond, aspect=2.2, align=center, inner sep=1pt, font=\small},
    arr/.style={-{Latex[length=2mm]}, thick}
  ]
    \node[box] (a) {Robot requests zone};
    \node[decision, below=of a] (b) {Zone\\occupied?};
    \node[box, below left=of b] (c) {Insert owner\\and enter zone};
    \node[box, below right=of b] (d) {Wait on\\condvar};
    \node[box, below=of c] (e) {Robot completes work};
    \node[box, below=of e] (f) {Release zone};
    \node[box, below=of f] (g) {Notify all\\waiters};

    \draw[arr] (a) -- (b);
    \draw[arr] (b) -- node[above left]{no} (c);
    \draw[arr] (b) -- node[above right]{yes} (d);
    \draw[arr] (d) -- ++(0,-7mm) -| (b.east);
    \draw[arr] (c) -- (e);
    \draw[arr] (e) -- (f);
    \draw[arr] (f) -- (g);
  \end{tikzpicture}
  \caption{Zone access control logic (TikZ version of the Mermaid flowchart).}
  \label{fig:zone-flow}
\end{figure}

\subsection{Health monitor}
The health monitor tracks per-robot heartbeats with a \texttt{Mutex}-protected \texttt{HashMap} and \texttt{HashSet}. \texttt{last\_seen} timestamps record the most recent heartbeat per robot, and \texttt{offline} records robots that have exceeded the timeout. The monitor can be polled at any interval; in the simulation a background thread performs periodic checks while robots run. The implementation relies on monotonic timestamps for timeout comparison \citep{rustinstant}.

\noindent\textbf{Critical code path (offline detection):}
\begin{lstlisting}
pub fn detect_offline(&self, timeout: Duration) -> HashSet<RobotId> {
    let mut guard = self.state.lock().expect("health monitor mutex poisoned");
    let now = Instant::now();
    let overdue: Vec<RobotId> = guard
        .last_seen
        .iter()
        .filter_map(|(&robot, &last)| {
            if now.duration_since(last) > timeout { Some(robot) } else { None }
        })
        .collect();
    for robot in overdue {
        guard.offline.insert(robot);
    }
    guard.offline.clone()
}
\end{lstlisting}

Tests ensure that robots are marked offline after timeouts and that a new heartbeat clears the offline status. A test-only hook allows deterministic verification without sleeping, preventing flaky test behavior.

\subsection{Simulation and CLI}

\begin{figure}[h]
  \centering
  \resizebox{\linewidth}{!}{%
  \begin{tikzpicture}[
    font=\small,
    lifeline/.style={draw, rounded corners, align=center, minimum width=24mm, minimum height=7mm},
    msg/.style={-{Latex[length=2mm]}, thick},
    dashedline/.style={densely dashed, gray}
  ]
    % Lifelines
    \node[lifeline] (r) {Robot\\thread};
    \node[lifeline, right=18mm of r] (q) {TaskQueue};
    \node[lifeline, right=18mm of q] (z) {ZoneAccess};
    \node[lifeline, right=18mm of z] (h) {HealthMonitor};
    \node[lifeline, right=18mm of h] (m) {Monitor\\thread};

    % Vertical lifelines (long enough for the last messages)
    \foreach \x in {r,q,z,h,m} {
      \draw[dashedline] (\x.south) -- ++(0,-72mm);
    }

    % Message y-levels (all measured from r.south)
    \coordinate (y1) at ($(r.south)+(0,-8mm)$);
    \coordinate (y2) at ($(r.south)+(0,-16mm)$);
    \coordinate (y3) at ($(r.south)+(0,-24mm)$);
    \coordinate (y4) at ($(r.south)+(0,-32mm)$);
    \coordinate (y5) at ($(r.south)+(0,-40mm)$);
    \coordinate (y6) at ($(r.south)+(0,-48mm)$);
    \coordinate (y7) at ($(r.south)+(0,-58mm)$);
    \coordinate (y8) at ($(r.south)+(0,-66mm)$);

    % Messages (adapted from Mermaid sequence diagram)
    \draw[msg] ($(r.south |- y1)$) -- node[above]{\texttt{pop\_blocking()}} ($(q.south |- y1)$);
    \draw[msg] ($(q.south |- y2)$) -- node[above]{Task} ($(r.south |- y2)$);

    \draw[msg] ($(r.south |- y3)$) -- node[above]{\texttt{acquire(zone)}} ($(z.south |- y3)$);
    \draw[msg] ($(z.south |- y4)$) -- node[above]{granted} ($(r.south |- y4)$);

    \draw[msg] ($(r.south |- y5)$) -- node[above]{\texttt{heartbeat()}} ($(h.south |- y5)$);
    \draw[msg] ($(r.south |- y6)$) -- node[above]{\texttt{release(zone)}} ($(z.south |- y6)$);

    \draw[msg] ($(m.south |- y7)$) -- node[above]{\texttt{detect\_offline(timeout)}} ($(h.south |- y7)$);
    \draw[msg] ($(h.south |- y8)$) -- node[above]{offline set} ($(m.south |- y8)$);
  \end{tikzpicture}%
  }
  \caption{Robot task execution and heartbeat monitoring sequence (TikZ version of the Mermaid diagram).}
  \label{fig:robot-sequence}
\end{figure}

The CLI (\texttt{main.rs}) provides three entry points: a default demo, a single benchmark run, and a stress sweep. The demo spawns three robot threads, loads a fixed number of tasks, and uses two zones to create contention. One robot intentionally stops sending heartbeats mid-run, and the background monitor thread logs the offline event. The demo ends with a summary that includes per-robot tasks, maximum zone occupancy observed, and the offline robot set. Shared ownership of the core services across threads is implemented with reference-counted pointers, the standard pattern for sharing immutable handles in Rust \citep{rustarc}.

Benchmarks and stress runs reuse the same core logic but collect metrics such as elapsed time, throughput, average zone wait time, CPU usage (when available), and validation flags (duplicate tasks, zone violations). These runs are designed for repeatable, observable output rather than high realism. Logging is disabled in release builds to avoid skewing timing measurements. Some tests and helper routines use standard library channels for deterministic coordination in multithreaded tests \citep{rustmpsc}.

\subsection{Synchronization strategy and invariants}
All shared mutable state is accessed under a single lock per component, avoiding multi-lock cycles and eliminating deadlock risks. The task queue and zone controller use condition variables to coordinate waiting without busy loops, while the health monitor uses a mutex to serialize heartbeat updates and offline detection. Atomics are used only for lightweight counters in the simulation and do not replace the fundamental locks that protect shared structures.

The design enforces the required safety invariants:
\begin{itemize}[leftmargin=*]
  \item Each task is removed from the queue at most once.
  \item A zone is owned by at most one robot at a time.
  \item Robots are marked offline if their heartbeat exceeds a timeout.
  \item Critical sections are short and consistent, with no nested locks.
\end{itemize}

\section{Benchmark}
\subsection{Methodology}
Benchmarks were run on 2026-02-01 on macOS (Darwin 25.2.0, arm64) using \texttt{rustc 1.93.0}. Each run used the release build (\texttt{cargo\allowbreak\ run\allowbreak\ --release\allowbreak\ --\allowbreak bench\allowbreak\ ...}) to minimize debug overhead. The benchmark harness assigns a fixed number of tasks to multiple robot threads, each of which acquires a zone, sleeps for \texttt{work\_ms}, and releases the zone. A monitor thread checks for offline robots when requested. Metrics include elapsed time, throughput (tasks/sec), average zone wait time (microseconds), CPU user/system time (Unix only), and validation flags for duplicate tasks and zone exclusivity.

Validation mode enables extra safety checks (duplicate task IDs, zone exclusivity flags) without changing the synchronization logic. Offline detection can also be enabled to force a robot to stop heartbeats mid-run, providing a concrete liveness signal in the metrics. CPU usage is derived from \texttt{getrusage} on Unix platforms and is reported as the delta between start and end of the benchmark. Because the workloads are short, CPU figures should be interpreted as indicative rather than definitive, and elapsed time is a more stable measure for comparison.

Before recording measurements, the test suite was executed with \texttt{cargo test}; all 12 tests (11 unit tests plus 1 CLI integration test) passed. This ensures the core correctness invariants hold prior to performance evaluation.

\subsection{Benchmark results (single-run samples)}
The table below shows representative single-run measurements.

\begin{table}[h]
\centering
\small
\setlength{\tabcolsep}{5pt}
\resizebox{\linewidth}{!}{%
\begin{tabular}{rrrrrrrrrrrcr}
\toprule
Robots & Tasks/robot & Zones & Work (ms) & Elapsed (ms) & Throughput & Avg wait (\(\mu\)s) & CPU user (s) & CPU sys (s) & Max occ. & Zone viol. & Offline \\
\midrule
4 & 25 & 2 & 5 & 315 & 317.46 & 5093.75 & 0.0006 & 0.0028 & 2 & false & 0 \\
8 & 25 & 2 & 5 & 619 & 323.10 & 15437.80 & 0.0015 & 0.0134 & 3 & true & 0 \\
8 & 25 & 4 & 5 & 414 & 483.09 & 5503.01 & 0.0010 & 0.0081 & 5 & true & 0 \\
4 & 25 & 2 & 5 & 725 & 137.93 & 3852.46 & 0.0004 & 0.0016 & 3 & true & 1 \\
\bottomrule
\end{tabular}%
}
\caption{Representative benchmark measurements (single runs).}
\label{tab:bench-samples}
\end{table}

\subsection{Scalability sweep}
A small stress sweep varying robots and zones (tasks/robot=10, work=5 ms) provides a quick scalability snapshot.

\begin{table}[h]
\centering
\small
\setlength{\tabcolsep}{6pt}
\begin{tabular}{rrrrrrc}
\toprule
Robots & Zones & Elapsed (ms) & Throughput & Avg wait (\(\mu\)s) & Max occ. & Zone viol. \\
\midrule
1 & 1 & 105 & 95.24 & 0.00 & 1 & false \\
1 & 2 & 105 & 95.24 & 0.00 & 1 & false \\
2 & 1 & 209 & 95.69 & 3068.05 & 1 & false \\
2 & 2 & 102 & 196.08 & 79.50 & 2 & false \\
4 & 1 & 315 & 126.98 & 10438.05 & 1 & false \\
4 & 2 & 210 & 190.48 & 4462.15 & 3 & true \\
\bottomrule
\end{tabular}
\caption{Scalability sweep (single runs).}
\label{tab:scalability}
\end{table}

\subsection{Graph (throughput by robots, zones=2)}
\begin{figure}[h]
\centering
\begin{lstlisting}[language={},basicstyle=\ttfamily\small,frame=single]
Robots: 1 | ################ (95)
Robots: 2 | ################################# (196)
Robots: 4 | ################################ (190)
Robots: 8 | ########################################### (323)
\end{lstlisting}
\caption{ASCII throughput visualization from a sample sweep (zones=2).}
\label{fig:ascii-throughput}
\end{figure}

\subsection{Correctness and stress observations}
Each data point above represents a single execution; short workloads are sensitive to scheduling noise, so the absolute values can vary across runs. However, the relative trends were stable: throughput increases with more robots and zones, while contention drives up zone wait time. The offline-enabled benchmark also shows a longer elapsed time because the harness waits for the timeout window to elapse before terminating the monitor thread, which is intentional for a deterministic offline signal.

\begin{itemize}[leftmargin=*]
  \item \textbf{Task safety:} No duplicate tasks were observed in any run; the validation flag remained false. This matches the queue's unit tests and indicates correct single-consumer behavior.
  \item \textbf{Zone exclusivity:} Some multi-robot runs reported \texttt{zone\_violation=true}, even though unit tests for \texttt{ZoneAccess} enforce single-zone occupancy. This suggests either a measurement artifact or a corner case in the benchmark instrumentation. The underlying locking logic remains the same across tests and benchmarks; further investigation is noted in Discussion.
  \item \textbf{Offline detection:} When offline demo mode is enabled, a robot that stops heartbeats is marked offline (offline robots = 1). The monitor's polling interval and timeout are short to keep the demo responsive.
\end{itemize}

Overall, throughput scales with additional robots and zones, while average zone wait grows when many robots contend for fewer zones. The simple design achieves predictable behavior and provides measurable evidence of synchronization under concurrency.

To further contextualize the numbers, the stress sweep shows that doubling zones from 1 to 2 nearly doubles throughput at low robot counts, reflecting reduced blocking. At higher robot counts, throughput gains taper, indicating that the task queue and the simulated work duration become the dominant limits. These observations align with the expected behavior of a mutex-protected resource manager: contention is the primary driver of queueing delay, and adding independent resources restores parallelism.

\section{Discussion}
The core goal of the project is correctness in concurrency, and the implementation meets this requirement by applying straightforward synchronization patterns. The task queue demonstrates a classic producer-consumer solution with a mutex and condition variable, yielding deterministic, single-consumer behavior. The zone controller models exclusive resource ownership, ensuring that a robot must explicitly acquire and release a zone, a pattern familiar from lock managers in systems and databases. The health monitor illustrates a watchdog-style approach to liveness detection and shows how timeout-driven logic can be implemented safely with mutex-protected state.

The results highlight predictable trade-offs. When the number of robots increases without a matching increase in zones, throughput improvements are limited by contention and by the time each robot holds a zone. This is visible in the higher average zone wait times for configurations with many robots and few zones. Conversely, adding zones improves throughput and reduces wait time, demonstrating that the synchronization scheme allows parallelism when resources are available. CPU usage remains low in these microbenchmarks because most of the wall time is spent sleeping to simulate work, which is appropriate for a coordination-focused simulation.

The benchmark output also surfaced an unexpected observation: several multi-robot configurations reported \texttt{zone\_violation=true}, even though the \texttt{ZoneAccess} unit tests show strict exclusivity. Because the zone controller itself is protected by a single mutex and uses condition-variable waiting, it should not allow two robots into the same zone. The most plausible explanations are (1) a measurement artifact in the benchmark counters, or (2) a rare edge case in the instrumentation around release and per-zone occupancy tracking. This discrepancy is valuable because it illustrates a common systems engineering lesson: instrumentation must be validated with the same rigor as the core logic. For production use, the benchmark counters should be upgraded to trace per-zone ownership transitions explicitly, or to assert that each acquire has a matching successful release before updating counters. In the current project scope, this does not compromise the core synchronization design, but it does limit how strongly the benchmark data can be used as evidence of exclusivity under heavy contention.

Another trade-off involves the heartbeat timeout. A shorter timeout improves detection speed but risks marking a slow robot offline. The demo uses short timeouts to make the offline event visible, whereas a real deployment would choose a longer threshold and possibly incorporate adaptive timing. The current design keeps this parameter explicit and configurable at the simulation level, which makes the timing assumptions transparent and easier to justify in a report.

From a software engineering perspective, Rust's ownership model and standard synchronization primitives promote safe concurrency. The use of \texttt{Arc} for shared ownership and \texttt{Mutex} for interior mutability avoids data races without requiring unsafe code. The design avoids nested locks, which eliminates deadlock cycles and keeps critical sections short. This aligns with the project's decision rule of correctness over performance and supports the learning outcome that synchronization should be explicit and simple when possible.

A limitation of the current evaluation is that benchmarks are short and focused on synchronization behavior rather than sustained load. Longer runs would reduce noise in throughput and CPU measurements, and repeated trials would allow confidence intervals. Additionally, the simulation uses a fixed work duration and deterministic task-to-zone mapping, which simplifies analysis but does not capture heterogeneous task costs or dynamic zone selection. These constraints are acceptable for the minimal scope but should be revisited if the system evolves toward more realistic scheduling. From a dependability perspective, the current evaluation focuses on safety and correctness rather than on formal reliability or availability metrics \citep{avizienis2004}.

Overall, the project demonstrates that minimal, well-structured synchronization can produce a correct and observable coordination layer. The system's behavior is easy to reason about, and the design choices map cleanly to OS concurrency principles. The remaining gaps are primarily in the rigor of benchmark instrumentation and in the breadth of performance evaluation, rather than in the correctness of the core synchronization mechanisms.

\section{Conclusion and Future Work}
Medical Care Robot Coordination System (MCRoCS) achieves its goal of implementing a lightweight OS core that safely coordinates multiple robot threads. It delivers the required task queue, zone access control, and health monitor with clear synchronization boundaries and minimal shared state. The demo shows multiple robots fetching tasks concurrently, exclusive zone occupancy, and an offline robot detected through heartbeat timeouts. Unit tests and an integration test provide direct evidence of correctness for each required component, and benchmark runs quantify throughput and waiting behavior under contention.

The project's minimal design was a deliberate choice to prioritize correctness and clarity. Using mutexes and condition variables makes the concurrency model explicit and easy to analyze, while avoiding deadlock and complex scheduling policies. The architecture is modular, with each component encapsulating its own synchronization, which simplifies reasoning and supports extension. These properties align with the learning outcomes for concurrency control, synchronization, and coordination.

Future work can build on this foundation in several directions. First, the benchmark instrumentation should be strengthened to eliminate false-positive exclusivity violations and to trace ownership transitions more precisely. Second, longer and repeated performance runs should be added to reduce measurement noise and to generate statistically meaningful metrics and graphs. Third, the simulation could incorporate heterogeneous task durations, dynamic zone selection, and injected delays to model more realistic operational conditions. Fourth, the health monitor could be extended with adaptive timeout policies or a suspicion window to balance detection speed with false-positive avoidance. Finally, if the project scope expands, it would be natural to explore advanced scheduling strategies or more scalable queue designs, while preserving the correctness guarantees that the minimal system already demonstrates.

In its current form, the system is already suitable for the required demo and report deliverables: it has a clear architecture, observable concurrency behavior, and a clean test suite that verifies the three mandatory components. These qualities make it a strong baseline for any future extensions.

\begin{thebibliography}{99}
\bibitem[Arpaci-Dusseau \& Arpaci-Dusseau(2023)]{arpaci2018} Arpaci-Dusseau, R. H., \& Arpaci-Dusseau, A. C. (2023). \textit{Operating Systems: Three Easy Pieces} (Version 1.10). Arpaci-Dusseau Books. Retrieved February 2, 2026, from https://pages.cs.wisc.edu/~remzi/OSTEP/
\bibitem[Birrell(1989)]{birrell1989} Birrell, A. D. (1989). \textit{An Introduction to Programming with Threads}. Digital Equipment Corporation.
\bibitem[Dijkstra(n.d.)]{dijkstra1965} Dijkstra, E. W. (n.d.). \textit{Cooperating sequential processes (EWD 123)}. Retrieved February 2, 2026, from https://www.cs.utexas.edu/~EWD/transcriptions/EWD01xx/EWD123.html
\bibitem[Downey(2016)]{downey2016} Downey, A. B. (2016). \textit{The Little Book of Semaphores} (2nd ed.). Green Tea Press.
\bibitem[Hoare(1974)]{hoare1974} Hoare, C. A. R. (1974). Monitors: An operating system structuring concept. \textit{Communications of the ACM, 17}(10), 549--557.
\bibitem[Lampson \& Redell(1980)]{lampson1980} Lampson, B. W., \& Redell, D. D. (1980). Experience with processes and monitors in Mesa. \textit{Communications of the ACM, 23}(2), 105--117.
\bibitem[Lamport(1978)]{lamport1978} Lamport, L. (1978). Time, clocks, and the ordering of events in a distributed system. \textit{Communications of the ACM, 21}(7), 558--565.
\bibitem[Herlihy et~al.(2020)]{herlihy2020} Herlihy, M., Shavit, N., Luchangco, V., \& Spear, M. (2020). \textit{The Art of Multiprocessor Programming} (2nd ed.). Morgan Kaufmann.
\bibitem[Silberschatz et~al.(2018)]{silberschatz2018} Silberschatz, A., Galvin, P. B., \& Gagne, G. (2018). \textit{Operating System Concepts} (10th ed.). Wiley.
\bibitem[Tanenbaum \& Bos(2015)]{tanenbaum2015} Tanenbaum, A. S., \& Bos, H. (2015). \textit{Modern Operating Systems} (4th ed.). Pearson.
\bibitem[Williams(2019)]{williams2019} Williams, A. (2019). \textit{C++ Concurrency in Action} (2nd ed.). Manning.
\bibitem[Klabnik \& Nichols(2022)]{klabnik2022} Klabnik, S., \& Nichols, C. (2022). \textit{The Rust Programming Language} (2nd ed.). No Starch Press.
\bibitem[Rust Project(n.d.-a)]{rustmutex} Rust Project. (n.d.). \textit{std::sync::Mutex}. Retrieved February 2, 2026, from https://doc.rust-lang.org/std/sync/struct.Mutex.html
\bibitem[Rust Project(n.d.-b)]{rustcondvar} Rust Project. (n.d.). \textit{std::sync::Condvar}. Retrieved February 2, 2026, from https://doc.rust-lang.org/std/sync/struct.Condvar.html
\bibitem[Rust Project(n.d.-c)]{rustarc} Rust Project. (n.d.). \textit{std::sync::Arc}. Retrieved February 2, 2026, from https://doc.rust-lang.org/std/sync/struct.Arc.html
\bibitem[Rust Project(n.d.-d)]{rustmpsc} Rust Project. (n.d.). \textit{std::sync::mpsc}. Retrieved February 2, 2026, from https://doc.rust-lang.org/std/sync/mpsc/index.html
\bibitem[Rust Project(n.d.-e)]{rustinstant} Rust Project. (n.d.). \textit{std::time::Instant}. Retrieved February 2, 2026, from https://doc.rust-lang.org/std/time/struct.Instant.html
\bibitem[Gray \& Reuter(1993)]{gray1993} Gray, J., \& Reuter, A. (1993). \textit{Transaction Processing: Concepts and Techniques}. Morgan Kaufmann.
\bibitem[Lynch(1996)]{lynch1996} Lynch, N. (1996). \textit{Distributed Algorithms}. Morgan Kaufmann.
\bibitem[Chandra \& Toueg(1996)]{chandra1996} Chandra, T. D., \& Toueg, S. (1996). Unreliable failure detectors for reliable distributed systems. \textit{Journal of the ACM, 43}(2), 225--267.
\bibitem[Avizienis et~al.(2004)]{avizienis2004} Avizienis, A., Laprie, J.-C., Randell, B., \& Landwehr, C. (2004). \textit{Basic concepts and taxonomy of dependable and secure computing} (Technical Report No. UMIACS-TR-2004-47). University of Maryland, Institute for Systems Research. Retrieved February 2, 2026, from https://drum.lib.umd.edu/items/6b297ffc-373b-404f-be3a-70cc849e21fd
\end{thebibliography}

\end{document}
